
spark-shell --master local[*] --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.3.2

------- batch dataframe

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val str1 = spark.read.textFile("orders.txt")

val str2 = str1.select(split(col("value"),",").alias("cp"))

spark.conf.set("spark.sql.shuffle.partitions",10)

val str3 = str2.select(str2.col("cp")(0).cast(TimestampType).alias("o_time"),
str2.col("cp")(2).cast(LongType).alias("clientID"),
str2.col("cp")(3).cast(StringType).alias("stock"),
str2.col("cp")(4).cast(LongType).alias("amount"),
str2.col("cp")(5).cast(DoubleType).alias("price"),
str2.col("cp")(6).cast(StringType).alias("o_type")
).withColumn("transact", when(col("o_type") === "S", lit(-1)*col("amount")*col("price")).otherwise(col("amount")*col("price")))

str3.groupBy("stock").agg(count("stock").alias("count"),sum("amount").alias("total")).orderBy(desc("count")).show


------- stream dataframe reading from local directory

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val str1 = spark.readStream.textFile("/root/orders")

val str2 = str1.select(split(col("value"),",").alias("cp"))

spark.conf.set("spark.sql.shuffle.partitions",10)

val str3 = str2.select(to_timestamp(str2.col("cp")(0)).alias("o_time"),
str2.col("cp")(2).cast(LongType).alias("clientID"),
str2.col("cp")(3).cast(StringType).alias("stock"),
str2.col("cp")(4).cast(LongType).alias("amount"),
str2.col("cp")(5).cast(DoubleType).alias("price"),
str2.col("cp")(6).cast(StringType).alias("o_type"))

val str4 = str3.groupBy("stock").agg(count("stock").alias("count"),sum("amount").alias("total")).orderBy(desc("count"))

str4.writeStream.format("console").outputMode("complete").start()


------- stream dataframe reading from kafka (every 30 secs)

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.streaming.Trigger

val str1 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "kfk1:9092,kfk2:9092,kfk3:9092").option("subscribe", "test").load()

val str2 = str1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").select(split(col("value"),",").alias("cp"))

spark.conf.set("spark.sql.shuffle.partitions",10)

val str3 = str2.select(to_timestamp(str2.col("cp")(0)).alias("o_time"),
str2.col("cp")(2).cast(LongType).alias("clientID"),
str2.col("cp")(3).cast(StringType).alias("stock"),
str2.col("cp")(4).cast(LongType).alias("amount"),
str2.col("cp")(5).cast(DoubleType).alias("price"),
str2.col("cp")(6).cast(StringType).alias("o_type")
).withColumn("transact", when(col("o_type") === "S", lit(-1)*col("amount")*col("price")).otherwise(col("amount")*col("price")))

val str4 = str3.groupBy("stock").agg(count("stock").alias("count"),sum("transact").alias("total")).orderBy(asc("total"))

str4.writeStream.trigger(Trigger.ProcessingTime("30 seconds")).format("console").option("numRows",10).outputMode("complete").start()

----------------

from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers='kfk1:9092,kfk2:9092,kfk3:9092')

import time
file1 = open('orders.txt', 'r')
Lines = file1.readlines()
  
count = 0
for line in Lines:
    count += 1
    producer.send('test', key=bytes(f'message-${count}',encoding="raw_unicode_escape"), value=bytes(line,encoding="raw_unicode_escape"))
    time.sleep(0.5)


------- stream dataframe reading from kafka (every 30 secs with 2 minute windowframe )

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.streaming.Trigger

val str1 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "kfk1:9092,kfk2:9092,kfk3:9092").option("subscribe", "test").load()

val str2 = str1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").select(split(col("value"),",").alias("cp"))

spark.conf.set("spark.sql.shuffle.partitions",10)

val str3 = str2.select(to_timestamp(str2.col("cp")(7)).alias("evt_time"),
str2.col("cp")(2).cast(LongType).alias("clientID"),
str2.col("cp")(3).cast(StringType).alias("stock"),
str2.col("cp")(4).cast(LongType).alias("amount"),
str2.col("cp")(5).cast(DoubleType).alias("price"),
str2.col("cp")(6).cast(StringType).alias("o_type")
).withColumn("transact", when(col("o_type") === "S", lit(-1)*col("amount")).otherwise(col("amount")))

val str4 = str3.withWatermark("evt_time", "2 minutes").groupBy(str3.col("stock"),window(col("evt_time"), "2 minute"))
.agg(count("stock").alias("count"),sum("transact").alias("total"))
.select("stock","count","total").orderBy(desc("total"))

val qry = str4.writeStream.trigger(Trigger.ProcessingTime("30 seconds")).format("console").option("numRows",10).outputMode("complete").start()

---------------


from datetime import datetime, timedelta
from random import randint, random

from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers='kfk1:9092,kfk2:9092,kfk3:9092')

import time
file1 = open('orders.txt', 'r')
Lines = file1.readlines()
  
count = 0
for line in Lines:
    secs = -1 * randint(0,100)
    evt_time = datetime.now() + timedelta(seconds=secs)
	
    count += 1
    producer.send('test', key=bytes(f'message-${count}',encoding="raw_unicode_escape"), value=bytes(line+','+evt_time.strftime('%Y-%m-%d %H:%M:%S'),encoding="raw_unicode_escape"))
    time.sleep(0.1)
